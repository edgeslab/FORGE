{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data to drop users and items with low number of interactions#\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(path, data):\n",
    "    f = open(path, 'w')\n",
    "    jsObj = json.dumps(data)\n",
    "    f.write(jsObj)\n",
    "    f.close()\n",
    "\n",
    "def dataset_filtering(interaction, core):\n",
    "    # filtering the dataset with core\n",
    "    # movielens is filtered by only remaining only users with at least 20 interactions\n",
    "    # we further filter the dataset by remaining users and items with at least 20 interactions\n",
    "    user_id_dic = {}  # record the number of interaction for each user and item\n",
    "    item_id_dic = {}\n",
    "    for (user_id, item_id) in interaction:\n",
    "        try:\n",
    "            user_id_dic[user_id] += 1\n",
    "        except:\n",
    "            user_id_dic[user_id] = 1\n",
    "        try:\n",
    "            item_id_dic[item_id] += 1\n",
    "        except:\n",
    "            item_id_dic[item_id] = 1\n",
    "    print ('#Original dataset')\n",
    "    print('  User:', len(user_id_dic), 'Item:', len(item_id_dic))\n",
    "    print('  User:', len(user_id_dic), 'Item:', len(item_id_dic), 'Interaction:', len(interaction), 'Sparsity:',\n",
    "          100 - len(interaction) * 100.0 / len(user_id_dic) / len(item_id_dic), '%')\n",
    "    sort_user = []\n",
    "    sort_item = []\n",
    "    for user_id in user_id_dic:\n",
    "        sort_user.append((user_id, user_id_dic[user_id]))\n",
    "    for item_id in item_id_dic:\n",
    "        sort_item.append((item_id, item_id_dic[item_id]))\n",
    "    sort_user.sort(key=lambda x: x[1])\n",
    "    sort_item.sort(key=lambda x: x[1])\n",
    "    print ('Fitering(core = ', core, '...', end = '')\n",
    "    while sort_user[0][1] < core or sort_item[0][1] < core:\n",
    "        # find out all users and items with less than core recorders\n",
    "        user_LessThanCore = set()\n",
    "        item_LessThanCore = set()\n",
    "        for pair in sort_user:\n",
    "            if pair[1] < core:\n",
    "                user_LessThanCore.add(pair[0])\n",
    "            else:\n",
    "                break\n",
    "        for pair in sort_item:\n",
    "            if pair[1] < core:\n",
    "                item_LessThanCore.add(pair[0])\n",
    "            else:\n",
    "                break\n",
    "        # reconstruct the interaction record, remove the cool one\n",
    "        interaction_filtered = []\n",
    "        for (user_id, item_id) in interaction:\n",
    "            if not (user_id in user_LessThanCore or item_id in item_LessThanCore):\n",
    "                interaction_filtered.append((user_id, item_id))\n",
    "        # update the record\n",
    "        interaction = interaction_filtered\n",
    "\n",
    "        user_id_dic = {}  # record the number of interaction for each user and item\n",
    "        item_id_dic = {}\n",
    "        for (user_id, item_id) in interaction:\n",
    "            try:\n",
    "                user_id_dic[user_id] += 1\n",
    "            except:\n",
    "                user_id_dic[user_id] = 1\n",
    "            try:\n",
    "                item_id_dic[item_id] += 1\n",
    "            except:\n",
    "                item_id_dic[item_id] = 1\n",
    "\n",
    "        sort_user = []\n",
    "        sort_item = []\n",
    "        for user_id in user_id_dic:\n",
    "            sort_user.append((user_id, user_id_dic[user_id]))\n",
    "        for item_id in item_id_dic:\n",
    "            sort_item.append((item_id, item_id_dic[item_id]))\n",
    "        sort_user.sort(key=lambda x: x[1])\n",
    "        sort_item.sort(key=lambda x: x[1])\n",
    "        print (len(interaction), end = ' ')\n",
    "    print()\n",
    "    print ('#Filtered dataset')\n",
    "    print ('  User:', len(user_id_dic), 'Item:', len(item_id_dic), 'Interaction:', len(interaction), 'Sparsity:', 100 - len(interaction) * 100.0 / len(user_id_dic) / len(item_id_dic), '%')\n",
    "    return interaction\n",
    "\n",
    "\n",
    "\n",
    "#read interaction data\n",
    "core = 80\n",
    "cold_thre = 15\n",
    "\n",
    "\n",
    "\n",
    "path_read_inter = '../../../MovieLense/ml-100k/u.data'\n",
    "f_inter = open(path_read_inter, \"r\")\n",
    "data_inter = f_inter.read()\n",
    "f_inter.close()\n",
    "Interaction = []\n",
    "\n",
    "data_inter = data_inter.split('\\n')\n",
    "for interactions in data_inter:\n",
    "    interactions = interactions.split('\\t')\n",
    "    if len(interactions) > 1:\n",
    "        Interaction.append((interactions[0], interactions[1]))\n",
    "\n",
    "Interaction = dataset_filtering(Interaction, core)\n",
    "\n",
    "\n",
    "remained_items=[]\n",
    "for el in range(len(Interaction)):\n",
    "    remained_items.append(Interaction[el][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find less popular movies\n",
    "\n",
    "dataset = pd.read_csv('../../../MovieLense/ml-100k/u.data', sep='\\t')\n",
    "dataset.head()\n",
    "\n",
    "movieDic={}\n",
    "movie_list=[]\n",
    "\n",
    "\n",
    "for i in range(len(Interaction)):    ##if want to decide the least popular after pruning cold users/items\n",
    "    movie_list.append(Interaction[i][1])\n",
    "\n",
    "res = Counter(movie_list)\n",
    "\n",
    "least_rated= res.most_common()[-70:] #1340 used to be 1405\n",
    "least_rated_movies=[]\n",
    "for i in range(len(least_rated)):\n",
    "    least_rated_movies.append((str(least_rated[i][0])))\n",
    "FemaleID_str= least_rated_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "\n",
    "#processs interaction data\n",
    "user_list= []\n",
    "item_list= []\n",
    "for i in range(len(Interaction)):\n",
    "    user_list.append(int(Interaction[i][0]))\n",
    "    item_list.append(int(Interaction[i][1]))\n",
    "user_set= sorted(set(user_list))\n",
    "item_set= sorted(set(item_list))\n",
    "\n",
    "\n",
    "D={} #dictionaey with key=user and value=items that user interacted with\n",
    "D = defaultdict(list)\n",
    "for j in range(len(Interaction)):\n",
    "    D[int(Interaction[j][0])].append(int(Interaction[j][1]))\n",
    "#print (len(D))\n",
    "\n",
    "#process user data and item data (dropped zipcode as it cannot be either scaler or onehotencoding)\n",
    "data_user= pd.read_csv('../../../MovieLense/ml-100k/u.user', sep='|', \n",
    "                  names=[\"user_id\", \"age\", \"gender\", \"job\",\"zipcode\"])\n",
    "data_user= data_user.drop(columns=['zipcode'])\n",
    "print (\"data_user\", data_user)\n",
    "\n",
    "data_item= pd.read_csv('../../../MovieLense/ml-100k/u.item', sep='|', names=[\"movie_id\", \"movie_title\", \"release_date\", \"video release_date\", \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"], encoding='latin-1')\n",
    "#print (\"data_item\",data_item)\n",
    "data_item= data_item.drop(columns=['movie_title', 'video release_date','IMDb URL'])\n",
    "data_item['release_date'] = data_item['release_date'].str[-4:]\n",
    "#print (\"data_item\",data_item)\n",
    "\n",
    "\n",
    "#add popularity column to data_item panda\n",
    "\n",
    "data_item['unpopular']= data_item['movie_id'].map(lambda x: 1 if str(x) in FemaleID_str else 0)\n",
    "print(data_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate user-item features\n",
    "\n",
    "DATA=[]\n",
    "feature_list=[]\n",
    "label_list=[]\n",
    "for user in sorted(random.sample(D.keys(), 150)):  #choose 100 users randomly\n",
    "    label_array=[]\n",
    "    for item in sorted(random.sample(item_set, 150)): #choose 100 items randomly\n",
    "        df_user= data_user.loc[data_user['user_id'] == user]\n",
    "        df_user= df_user.reset_index(drop=True)\n",
    "        df_item= data_item.loc[data_item['movie_id'] == item]\n",
    "        df_item= df_item.reset_index(drop=True)\n",
    "        feature_list.append(pd.concat([df_user,df_item], axis=1))\n",
    "\n",
    "        if item in D[user]:\n",
    "            label_array.append(1)  \n",
    "        else:\n",
    "            label_array.append(0)\n",
    "    label_list.append(np.array(label_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = pd.concat(feature_list) #change the feature list that contains multiple dataframes, to a single dataframe with single head and multiplee rows(vertical) \n",
    "feature_list= feature_list.drop(columns=['user_id', 'movie_id'])\n",
    "preprocess = make_column_transformer((StandardScaler(),['age','release_date']), \n",
    "                                     (OneHotEncoder(sparse=False), ['gender', 'job', 'unpopular']), remainder='passthrough')\n",
    "\n",
    "feature_list = preprocess.fit_transform(feature_list)\n",
    "feature_list= np.split(feature_list, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data based on interaction (not user)\n",
    "\n",
    "data_X = []\n",
    "data_Y = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "docID_test = []\n",
    "\n",
    "for u in range(len(label_list)):\n",
    "    all_index = list(range(len(label_list[u])))\n",
    "    train_index = set(random.sample(all_index, int(0.8* len(all_index))))\n",
    "    test_index = list(set(all_index) - train_index)\n",
    "    train_index = list(train_index)\n",
    "    data_X.append(feature_list[u][train_index])\n",
    "    data_Y.append(label_list[u][train_index])\n",
    "    test_X.append(feature_list[u][test_index])\n",
    "    test_Y.append(label_list[u][test_index])\n",
    "    docID_test.append(np.take(item_set, test_index).tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove qid with sum_female=0 or sum_male=0\n",
    "F_rel=0\n",
    "M_rel=0\n",
    "unwanted_u=[]\n",
    "\n",
    "for u in range(len(docID_test)):\n",
    "    #print (\"u\", u)\n",
    "    F_rel=0\n",
    "    M_rel=0\n",
    "    for doc in range(len(docID_test[0])):\n",
    "        #print (\"doc\", doc)\n",
    "        if str(docID_test[u][doc]) in least_rated_movies:\n",
    "            #print (\"True\")\n",
    "            F_rel= F_rel+ test_Y[u][doc]  \n",
    "        else:\n",
    "            M_rel= M_rel+ test_Y[u][doc]\n",
    "            #print (\"False\")\n",
    "    if F_rel==0 or M_rel==0:\n",
    "        print (\"F_rel\",F_rel)\n",
    "        print (\"M_rel\",M_rel)\n",
    "        unwanted_u.append(u)\n",
    "\n",
    "test_X_ = []\n",
    "test_Y_ = []\n",
    "docID_test_=[]\n",
    "for i in range(len(test_X)):\n",
    "    if i not in unwanted_u:\n",
    "        test_X_.append(test_X[i])\n",
    "        test_Y_.append(test_Y[i])\n",
    "        docID_test_.append(docID_test[i])\n",
    "test_X= test_X_\n",
    "test_Y= test_Y_\n",
    "docID_test= docID_test_\n",
    "\n",
    "\n",
    "pickle.dump(docID_test, open( \"MovieLense_docID_test_25.txt\", \"wb\" ) )\n",
    "\n",
    "pkl.dump((data_X, data_Y), open(\"MovieLense_train_rank_25.pkl\", \"wb\"))\n",
    "pkl.dump((test_X, test_Y), open(\"MovieLense_test_rank_25.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "docID_test = pickle.load( open( \"MovieLense_docID_test_25.txt\", \"rb\" ) )\n",
    "\n",
    "with open('MovieLense_train_rank_25.pkl', 'rb') as f:\n",
    "    data_train = pkl.load(f)\n",
    "\n",
    "    \n",
    "with open('MovieLense_test_rank_25.pkl', 'rb') as f:\n",
    "     data_test = pickle.load(f)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn rel scores\n",
    "\n",
    "from YahooDataReader import YahooDataReader\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "dr = YahooDataReader(None)\n",
    "dr.data = pkl.load(open(\"MovieLense_train_rank_25.pkl\", \"rb\"))\n",
    "vdr = YahooDataReader(None)\n",
    "vdr.data = pkl.load(open(\"MovieLense_test_rank_25.pkl\",\"rb\"))                  \n",
    "model = linear_model.LinearRegression(fit_intercept=False, normalize=False)\n",
    "feats, rel = dr.data\n",
    "feats = np.array([item for sublist in feats for item in sublist])\n",
    "rel = np.array([item for sublist in rel for item in sublist])\n",
    "model.fit(feats, rel)\n",
    "feats, rel = vdr.data\n",
    "se_sum = 0\n",
    "length = 0\n",
    "predicted_rels = []\n",
    "for i, query in enumerate(feats):\n",
    "    rel_pred = model.predict(query)\n",
    "    predicted_rels.append(rel_pred)\n",
    "    se_sum += np.sum((rel_pred - rel[i])**2)\n",
    "    length += len(rel[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docID_test_str=[[] for _ in range(len(docID_test))] #type=str\n",
    "size_female_test = []\n",
    "size_male_test=[]\n",
    "sum_rel_female=[]\n",
    "sum_rel_male= []\n",
    "sum_rel_totall=[]\n",
    "sum_rel_female_predicted=[]\n",
    "sum_rel_male_predicted= []\n",
    "sum_rel_totall_predicted=[]\n",
    "\n",
    "\n",
    "for qid in range(len(docID_test)):\n",
    "    F_size= 0\n",
    "    M_size= 0\n",
    "    F_rel=0\n",
    "    M_rel=0\n",
    "    F_rel_predicted=0\n",
    "    M_rel_predicted=0\n",
    "    for doc in range(len(docID_test[0])):\n",
    "        docID_test_str[qid].append(str(docID_test[qid][doc]))\n",
    "        if str(docID_test[qid][doc]) in FemaleID_str:\n",
    "            F_size=F_size+1\n",
    "            F_rel= F_rel+ data_test[1][qid][int (float(docID_test_str[qid][doc]))] # used only for evaluation (ML1M)\n",
    "            F_rel_predicted= F_rel_predicted+ predicted_rels[qid][int (float(docID_test_str[qid][doc]))] #used in FORGE and LinkedIn algo (ML1M)\n",
    "            \n",
    "        else:\n",
    "            M_size= M_size+1\n",
    "            M_rel= M_rel+ data_test[1][qid][int (float(docID_test_str[qid][doc]))]\n",
    "            M_rel_predicted= M_rel_predicted+ predicted_rels[qid][int (float(docID_test_str[qid][doc]))] \n",
    "    \n",
    "    size_female_test.append(F_size) \n",
    "    size_male_test.append(M_size)\n",
    "    totall_size= len (docID_test_str[qid])\n",
    "    \n",
    "    sum_rel_female.append(F_rel) \n",
    "    sum_rel_male.append(M_rel)\n",
    "    sum_rel_totall.append(sum_rel_female[qid]+ sum_rel_male[qid])\n",
    "    \n",
    "    sum_rel_female_predicted.append(F_rel_predicted) \n",
    "    sum_rel_male_predicted.append(M_rel_predicted)\n",
    "    sum_rel_totall_predicted.append(sum_rel_female_predicted[qid]+ sum_rel_male_predicted[qid])\n",
    "\n",
    "for i in range(len(np.divide(sum_rel_female, size_female_test))):\n",
    "    if np.divide(sum_rel_female_predicted, size_female_test)[i] > np.divide(sum_rel_male_predicted, size_male_test)[i]:\n",
    "        print (\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_Divergence(p1,p2,q1,q2): #can use jenson divergance that is bounded between 0 and 1#\n",
    "    if p1!=0 and p2!=0:\n",
    "        J=(    (p1*np.log((2*p1)/(p1+q1)))  +  (p2*np.log((2*p2)/(p2+q2)))    +  (q1*np.log((2*q1)/(p1+q1)))  +  (q2*np.log((2*q2)/(p2+q2)))    ) / 2\n",
    "    elif p1==0:\n",
    "        J=(    (p2*np.log((2*p2)/(p2+q2)))    +  (q1*np.log((2*q1)/(p1+q1)))  +  (q2*np.log((2*q2)/(p2+q2)))    ) / 2\n",
    "    elif p2==0:\n",
    "        J=(   (p1*np.log((2*p1)/(p1+q1)))  +  (q1*np.log((2*q1)/(p1+q1)))  +  (q2*np.log((2*q2)/(p2+q2)))    ) / 2\n",
    "    else:\n",
    "        J=(   (q1*np.log((2*q1)/(p1+q1)))  +  (q2*np.log((2*q2)/(p2+q2)))   )/ 2 \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness(size_docID, \n",
    "             size_female, \n",
    "             size_male, \n",
    "             current_male_CTR, \n",
    "             sofar_male_CTR, \n",
    "             current_female_CTR, \n",
    "             sofar_female_CTR,\n",
    "             ):\n",
    "\n",
    "\n",
    "    CTR_male = sofar_male_CTR + current_male_CTR\n",
    "    CTR_female =  sofar_female_CTR + current_female_CTR\n",
    "    CTR_total = CTR_male + CTR_female\n",
    "    current_fairness= 1- KL_Divergence(CTR_male / CTR_total,\n",
    "                                       CTR_female / CTR_total,\n",
    "                                       size_male/size_docID, \n",
    "                                       size_female/size_docID\n",
    "                                       )\n",
    "    return current_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_and_sort(\n",
    "    docID,\n",
    "    rel_scores\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Arg list:\n",
    "    movieID: list of movie IDs. Here, list of all docID for a certain qid\n",
    "    rel_scores: dict() that maps movie_ID -> rel_score\n",
    "    \"\"\"\n",
    "    female_sorted_by_rel = []\n",
    "    male_sorted_by_rel = []\n",
    "    \n",
    "    for i in docID:    \n",
    "        if i in FemaleID_str:\n",
    "            female_sorted_by_rel.append(i)\n",
    "        else:\n",
    "            male_sorted_by_rel.append(i)\n",
    "\n",
    "    female_sorted_by_rel.sort(key = lambda x : rel_scores[x], reverse=True) \n",
    "    male_sorted_by_rel.sort(key = lambda x : rel_scores[x], reverse=True)\n",
    "\n",
    "    return male_sorted_by_rel, female_sorted_by_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpolation_optimized(\n",
    "    qid,\n",
    "    docID,\n",
    "    rel_scores,\n",
    "    Z,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arg list:\n",
    "    docID: list of docIDs for each qid in test data\n",
    "    rel_scores: dict() that maps doc_ID -> rel_score\n",
    "    Z: Z value for interpolation\n",
    "    \"\"\"\n",
    "    all_sorted_by_rel= sorted(docID, key = lambda x : rel_scores[x], reverse=True)\n",
    "    S=[]\n",
    "    sofar_female_CTR=0\n",
    "    sofar_male_CTR=0\n",
    "    sofar_DCG=0\n",
    "    IDCG=0\n",
    "    \n",
    "    availablity = all_sorted_by_rel[:] #make a copy of all_sorted to avoid del make problem for IDCG\n",
    "    while len (S)< 30:\n",
    "        IDCG= IDCG+ (float(2**float(rel_scores[all_sorted_by_rel[len(S)]])-1) /  math.log2(1+len(S)+1))\n",
    "        epsilon_1= 0.65\n",
    "        epsilon_plus= 1- ((len(S)+2)/100)\n",
    "        epsilon_minus= epsilon_1* (1/min( len(S)+1, 10) )\n",
    "        max_intpol_score = 0\n",
    "        max_item_data = None\n",
    "        for item in availablity:\n",
    "            DCG = sofar_DCG+ (float(2**float(rel_scores[item])-1) /  math.log2(1+len(S)+1))\n",
    "            nDCG =  float (DCG)/ IDCG\n",
    "            if item in FemaleID_str:\n",
    "                current_male_CTR = 0\n",
    "                current_female_CTR = float(  (rel_scores[item]* epsilon_plus) + ( (1-rel_scores[item]) * epsilon_minus)  ) * 1/math.log2(1+len(S)+1)\n",
    "            else:\n",
    "                current_male_CTR =   float(  (rel_scores[item]* epsilon_plus) + ( (1-rel_scores[item]) * epsilon_minus)  ) * 1/math.log2(1+len(S)+1)\n",
    "                current_female_CTR = 0\n",
    "            fair_metric = fairness(sum_rel_totall_predicted[qid], sum_rel_female_predicted[qid], sum_rel_male_predicted[qid], current_male_CTR, sofar_male_CTR, current_female_CTR, sofar_female_CTR)\n",
    "            intpol_score = (1-Z) * nDCG + Z * fair_metric\n",
    "            if intpol_score > max_intpol_score:\n",
    "                max_intpol_score = intpol_score\n",
    "                max_item_data = (item, current_female_CTR, current_male_CTR, DCG)\n",
    "\n",
    "\n",
    "\n",
    "        S.append(max_item_data[0])\n",
    "        availablity.remove(max_item_data[0])\n",
    "        sofar_female_CTR += max_item_data[1]\n",
    "        sofar_male_CTR += max_item_data[2]\n",
    "        sofar_DCG = max_item_data[3]\n",
    "        \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_nDCG(docID, rel_scores, item_list):\n",
    "    \"\"\"\n",
    "    gives the nDCG of ranking\n",
    "    input:\n",
    "    docID: id of documents\n",
    "    rel_scores: relevant scores assuming predict = ground truth or not\n",
    "    item_list: dictionary key= docID, value= relscores\n",
    "    \"\"\"\n",
    "    sorted_docID= sorted(docID, key=lambda x:rel_scores [int(x)] , reverse=True) #for ML1M\n",
    "    #print (\"sorted_docID\", sorted_docID)\n",
    "    Denom= float(0)\n",
    "    Nom= float(0)\n",
    "    for i in range (len(item_list)):\n",
    "        temp1= D_real[str(sorted_docID[i])] #D1 if want to assume predicted rel is ground truth, D_real otherwise#\n",
    "        temp2= 2**(float (temp1))\n",
    "        Denom= Denom+(   (temp2-1)  / (math.log2(i+2))    )\n",
    "        temp3= D_real[str(item_list[i])]    #D1 if want to assume predicted rel is ground truth, D_real otherwise#\n",
    "        temp4= 2**(float (temp3))\n",
    "        Nom=Nom + (  (temp4-1)  / (math.log2(i+2))    )\n",
    "    nDCG= (float(Nom)/float(Denom))\n",
    "    return (nDCG, sorted_docID  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divergence (qid, docID, rel_scores):\n",
    "    sofar_female_CTR=0\n",
    "    sofar_male_CTR=0\n",
    "    D_movie_list=[]\n",
    "    D_male_list=[]\n",
    "    D_female_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        epsilon_1= 0.65\n",
    "        epsilon_plus= 1- ((i+2)/100)\n",
    "        epsilon_minus= epsilon_1* ( 1/min (i+1, 10) )\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_CTR += float(  (rel_scores[docID[i]]* epsilon_plus) + ( (1-rel_scores[docID[i]]) * epsilon_minus)  ) * 1/math.log2(i+2)\n",
    "        else:\n",
    "            sofar_male_CTR +=   float(  (rel_scores[docID[i]]* epsilon_plus) + ( (1-rel_scores[docID[i]]) * epsilon_minus)  ) * 1/math.log2(i+2)\n",
    "        \n",
    "        CTR_total= sofar_female_CTR+ sofar_male_CTR  \n",
    "        D_movie_list.append(KL_Divergence(sofar_male_CTR/ CTR_total, sofar_female_CTR/ CTR_total , sum_rel_male[qid]/sum_rel_totall[qid], sum_rel_female[qid]/sum_rel_totall[qid]))\n",
    "        D_male_list.append(float((sofar_male_CTR/ CTR_total)/(sum_rel_male[qid]/sum_rel_totall[qid])))\n",
    "        D_female_list.append(float((sofar_female_CTR/ CTR_total)/(sum_rel_female[qid]/sum_rel_totall[qid])))\n",
    "    return (D_movie_list, D_male_list, D_female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divergence_trust_DP (qid, docID, rel_scores):\n",
    "    sofar_female_CTR=0\n",
    "    sofar_male_CTR=0\n",
    "    D_movie_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        epsilon_1= 0.65\n",
    "        epsilon_plus= 1- ((i+2)/100)\n",
    "        epsilon_minus= epsilon_1* ( 1/min (i+1, 10) )\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_CTR += float(  (rel_scores[docID[i]]* epsilon_plus) + ( (1-rel_scores[docID[i]]) * epsilon_minus)  ) * 1/math.log2(i+2)\n",
    "        else:\n",
    "            sofar_male_CTR +=   float(  (rel_scores[docID[i]]* epsilon_plus) + ( (1-rel_scores[docID[i]]) * epsilon_minus)  ) * 1/math.log2(i+2)\n",
    "        \n",
    "        CTR_total= sofar_female_CTR+ sofar_male_CTR  \n",
    "        D_movie_list.append(KL_Divergence(sofar_male_CTR/ CTR_total, sofar_female_CTR/ CTR_total , size_male_test[qid]/totall_size, size_female_test[qid]/totall_size))\n",
    "    return (D_movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divergence_pos_DT (qid, docID, rel_scores):\n",
    "    sofar_female_exp=0\n",
    "    sofar_male_exp=0\n",
    "    D_movie_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_exp +=  1/math.log2(i+2)\n",
    "        else:\n",
    "            sofar_male_exp +=  1/math.log2(i+2)\n",
    "        \n",
    "        exp_total= sofar_female_exp+ sofar_male_exp  \n",
    "        D_movie_list.append(KL_Divergence(sofar_male_exp/ exp_total, sofar_female_exp/ exp_total , sum_rel_male[qid]/sum_rel_totall[qid], sum_rel_female[qid]/sum_rel_totall[qid]))\n",
    "    return (D_movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divergence_pos_DP (qid, docID, rel_scores):\n",
    "    sofar_female_exp=0\n",
    "    sofar_male_exp=0\n",
    "    D_movie_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_exp +=  1/math.log2(i+2)\n",
    "        else:\n",
    "            sofar_male_exp +=  1/math.log2(i+2)\n",
    "        \n",
    "        exp_total= sofar_female_exp+ sofar_male_exp  \n",
    "        D_movie_list.append(KL_Divergence(sofar_male_exp/ exp_total, sofar_female_exp/ exp_total , size_male_test[qid]/totall_size, size_female_test[qid]/totall_size))\n",
    "    return (D_movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is fairness definition of joachims (difference) based on disparate treatment\n",
    "\n",
    "def get_joachims_diff (qid, docID):  #gives a vector that has conv at each rank indx. can feed both item_list and sorted_item_list#\n",
    "    \n",
    "        \n",
    "    sofar_female_expo=0\n",
    "    sofar_male_expo=0\n",
    "    sofar_indx_expo=0\n",
    "    Diff_movie_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_expo= sofar_female_expo+(1/math.log2(i+2)) \n",
    "        else:\n",
    "            sofar_male_expo= sofar_male_expo + (1/math.log2(i+2))   \n",
    "        sofar_indx_expo= sofar_indx_expo+ (1/math.log2(i+2))\n",
    "        if sum_rel_male[qid]/size_male_test[qid] > sum_rel_female[qid]/size_female_test[qid]:\n",
    "            sign= 1\n",
    "        else:\n",
    "            sign=-1\n",
    "        Diff_movie_list.append(max(0, sign*diff(sofar_male_expo/ size_male_test[qid], sofar_female_expo/ size_female_test[qid] , sum_rel_male[qid]/size_male_test[qid], sum_rel_female[qid]/size_female_test[qid])))\n",
    "\n",
    "    return (Diff_movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CPFair_diff (qid, docID):  #gives a vector that has conv at each rank indx. can feed both item_list and sorted_item_list#\n",
    "    \n",
    "        \n",
    "    sofar_female_count=0\n",
    "    sofar_male_count=0\n",
    "    Diff_movie_list=[]\n",
    "    for i in range(len(docID)):\n",
    "        if docID[i] in FemaleID_str:\n",
    "            sofar_female_count= sofar_female_count+1 \n",
    "        else:\n",
    "            sofar_male_count= sofar_male_count + 1   \n",
    "        Diff_movie_list.append(abs(sofar_male_count- sofar_female_count))\n",
    "\n",
    "    return (Diff_movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FairMetric(DocID, DivOrDiff_item_list):    #do not need it if we have new trust and CTR\n",
    "    Denom=float(0)\n",
    "    Nom=float(0)\n",
    "    for i in range (len(DocID)):\n",
    "        Denom= Denom+ (1/ (math.log2(i+2)))\n",
    "        Nom= Nom+ (1/ (math.log2(i+2)))*DivOrDiff_item_list[i]   \n",
    "    n_item_list= float(Nom)/ float(Denom)\n",
    "    return (n_item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "interplotion_value=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92, 0.94, 0.96, 0.98, 0.99, 1]\n",
    "for z in interplotion_value:\n",
    "    D2={} # a dictionary: key= user id, value= S/item_list\n",
    "\n",
    "    nDCG_all_q=[]\n",
    "    D_item_list_all_q=[]\n",
    "    male_Div_all_q=[]\n",
    "    female_Div_all_q=[]\n",
    "    \n",
    "    D_item_list_trust_DP_all_q=[]\n",
    "    D_item_list_pos_DT_all_q=[]\n",
    "    D_item_list_pos_DP_all_q=[]\n",
    "    joachims_diff_item_list_all_q=[]\n",
    "    CPFair_diff_item_list_all_q=[]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    for qid in range(len(docID_test)):\n",
    "        nDCG=[]\n",
    "        nDiv=[]\n",
    "        D1={} # a dictionary: key= movieID, value= predicted rel_scores\n",
    "        D_real= {} # a dictionary: key= movieID, value= true rel_scores\n",
    "        for i in range (len(docID_test[0])):\n",
    "            #D1[docID_test_str[qid][i]]= predicted_rels[qid][i] # for German Compas Comcast and ML100K\n",
    "            #D_real[docID_test_str[qid][i]]= data_test[1][qid][i] # for German Compas Comcast and ML100K\n",
    "            \n",
    "            D1[docID_test_str[qid][i]]= predicted_rels[qid][int (float(docID_test_str[qid][i]))] # for ML1M\n",
    "            D_real[docID_test_str[qid][i]]= data_test[1][qid][int (float(docID_test_str[qid][i]))] # for ML1M\n",
    "\n",
    "        item_list  = interpolation_optimized (qid,docID_test_str[qid], D1, z)\n",
    "        for i in range(len(item_list)):\n",
    "            nDCG.append(get_nDCG(docID_test[qid], data_test[1][qid], item_list[0:i+1])[0]) #predicted_rels[qid] if assume precietd rel is ground truth, data_test[1][qid] Otherwise\n",
    "            D_item_list= get_divergence(qid, item_list[0:i+1], D_real)[0]  #D1 if we assume predicted=true rel, and D_real o.w\n",
    "            male_Div= get_divergence(qid, item_list[0:i+1], D_real)[1]\n",
    "            female_Div= get_divergence(qid, item_list[0:i+1], D_real)[2]\n",
    "            \n",
    "            D_item_list_trust_DP= get_divergence_trust_DP (qid, item_list[0:i+1], D_real)\n",
    "            D_item_list_pos_DT= get_divergence_pos_DT (qid, item_list[0:i+1], D_real)\n",
    "            D_item_list_pos_DP= get_divergence_pos_DP (qid, item_list[0:i+1], D_real)\n",
    "            joachims_diff_item_list= get_joachims_diff (qid, item_list[0:i+1])\n",
    "            CPFair_diff_item_list= get_CPFair_diff (qid, item_list[0:i+1])\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        nDCG_all_q.append (nDCG)\n",
    "        D_item_list_all_q.append(D_item_list)\n",
    "        male_Div_all_q.append(male_Div)\n",
    "        female_Div_all_q.append(female_Div)\n",
    "        \n",
    "        D_item_list_trust_DP_all_q.append(D_item_list_trust_DP)\n",
    "        D_item_list_pos_DT_all_q.append(D_item_list_pos_DT)\n",
    "        D_item_list_pos_DP_all_q.append(D_item_list_pos_DP)\n",
    "        joachims_diff_item_list_all_q.append(joachims_diff_item_list)\n",
    "        CPFair_diff_item_list_all_q.append(CPFair_diff_item_list)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    avg_nDCG_at_indx_all_q= np.mean(nDCG_all_q, axis=0)\n",
    "    avg_D_item_list_at_indx_all_q= np.mean(D_item_list_all_q, axis=0)\n",
    "    avg_male_Div_at_indx_all_q= np.mean(male_Div_all_q, axis=0)\n",
    "    avg_female_Div_at_indx_all_q= np.mean(female_Div_all_q, axis=0)\n",
    "    \n",
    "    avg_D_item_list_trust_DP_at_indx_all_q= np.mean(D_item_list_trust_DP_all_q, axis=0)\n",
    "    avg_D_item_list_pos_DT_at_indx_all_q= np.mean(D_item_list_pos_DT_all_q, axis=0)\n",
    "    avg_D_item_list_pos_DP_at_indx_all_q= np.mean(D_item_list_pos_DP_all_q, axis=0)\n",
    "    avg_joachims_diff_item_list_at_indx_all_q= np.mean(joachims_diff_item_list_all_q, axis=0)\n",
    "    avg_CPFair_diff_item_list_at_indx_all_q= np.mean(CPFair_diff_item_list_all_q, axis=0)\n",
    "    \n",
    "\n",
    "    print (\"K=5\")\n",
    "    print (avg_nDCG_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_D_item_list_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_male_Div_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_female_Div_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_D_item_list_trust_DP_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_D_item_list_pos_DT_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_D_item_list_pos_DP_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_joachims_diff_item_list_at_indx_all_q[4], end='\\t')\n",
    "    print (avg_CPFair_diff_item_list_at_indx_all_q[4], end='\\t')\n",
    "    print (\"********************\")\n",
    "    print (\"********************\")\n",
    "    print (\"K=10\")\n",
    "    print ( avg_nDCG_at_indx_all_q[9], end='\\t')\n",
    "    print ( avg_D_item_list_at_indx_all_q[9], end='\\t')\n",
    "    print ( avg_male_Div_at_indx_all_q[9], end='\\t')\n",
    "    print ( avg_female_Div_at_indx_all_q[9], end='\\t')\n",
    "    print (avg_D_item_list_trust_DP_at_indx_all_q[9], end='\\t')\n",
    "    print (avg_D_item_list_pos_DT_at_indx_all_q[9], end='\\t')\n",
    "    print (avg_D_item_list_pos_DP_at_indx_all_q[9], end='\\t')\n",
    "    print ( avg_joachims_diff_item_list_at_indx_all_q[9], end='\\t')\n",
    "    print ( avg_CPFair_diff_item_list_at_indx_all_q[9], end='\\t')\n",
    "    print (\"********************\")\n",
    "    print (\"********************\")\n",
    "    print (\"K=30\")\n",
    "    print ( avg_nDCG_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_D_item_list_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_male_Div_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_female_Div_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_D_item_list_trust_DP_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_D_item_list_pos_DT_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_D_item_list_pos_DP_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_joachims_diff_item_list_at_indx_all_q[29], end='\\t')\n",
    "    print ( avg_CPFair_diff_item_list_at_indx_all_q[29], end='\\t')\n",
    "    print (\"********************\")\n",
    "    print (\"********************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
